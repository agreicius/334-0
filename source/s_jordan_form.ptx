<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="s_jordan_form" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Jordan form</title>

    <definition xml:id="d_jordan_block">
        <title>Jordan block</title>

        <statement>
            <p>
                Let <m>F</m> be a field.
                Given <m>\lambda\in F</m> and positive integer <m>k</m>, the <m>k\times k</m> matrix
                <me>
                    J=\begin{bmatrix} \lambda \amp 1\amp 0 \amp \dots \amp 0\\ 0 \amp \lambda\amp 1 \amp \\ \vdots \amp  \amp \ddots \amp \ddots \\ \amp \amp \amp \amp 1\\ 0\amp \dots \amp \amp \amp \lambda \end{bmatrix}
                </me>
                is called a <term><m>\lambda</m>-Jordan block</term>.
            </p>
        </statement>
    </definition>

    <definition xml:id="d_jordan_basis">
        <title>Jordan basis</title>

        <statement>
            <p>
                Let <m>T\in \mathcal{L}(V)</m>, where <m>V</m> is finite dimensional.
                A <term>Jordan basis</term> of <m>T</m> is a basis <m>B</m> for which <m>A=[T]_{B}</m> is block diagonal of the form
                <me>
                    A=\begin{bmatrix}J_{1}\amp \\ \amp J_{2}\amp \\ \amp \amp \ddots \\ \amp \amp \amp J_{r} \end{bmatrix}
                </me>
                where each <m>J_{i}</m> is a <m>\lambda_i</m>-Jordan block for some <m>\lambda_i\in F</m>.
                The matrix <m>A</m> is called a <term>Jordan form</term> of <m>T</m> in this case.
            </p>
        </statement>
    </definition>

    <definition xml:id="d_cycle_gen_eigenvectors">
        <title>Cycle of generalized eigenvectors</title>

        <statement>
            <p>
                Let <m>T\in \mathcal{L}(V)</m> where <m>V</m> is finite dimensional, and let <m>\lambda</m> be an eigenvalue of <m>T</m>.
                A <term>cycle of generalized eigenvectors</term> of <m>T</m> corresponding to <m>\lambda</m> is a tuple of the form
                <me>
                    \left((T-\lambda I)^{k-1}(\boldv), (T-\lambda I)^{k-2}(\boldv), \dots, (T-\lambda I)(\boldv), \boldv\right)
                </me>
                where <m>\boldv\ne \boldzero</m> and <m>k</m> is the smallest positive integer satisfying <m>(T-\lambda I)^{k}(\boldv)=\boldzero</m>.
                The vector <m>(T-\lambda I)^{k-1}(\boldv)</m> is called the <term>initial vector</term> of the cycle; the vector <m>\boldv</m> is called the <term>end vector</term> of the cycle.
            </p>
        </statement>
    </definition>

    <theorem xml:id="th_jordan_cycles">
        <title>Jordan blocks</title>

        <statement>
            <p>
                Let <m>T\in \mathcal{L}(V)</m> where <m>V</m> is finite dimensional.
                <ol>
                    <li>
                        <m>[T]_{B}</m> is a Jordan block if and only if <m>B</m> is a cycle of generalized eigenvectors.
                    </li>

                    <li>
                        <m>[T]_{B}</m> is a Jordan form if and only if <m>B</m> is a concatenation <m>B=B_1*B_2*\cdots * B_r</m> of disjoint cycles of generalized eigenvectors.
                    </li>

                    <li>
                        A cycle of generalized eigenvectors is linearly independent, and its initial vector is an eigenvector.
                    </li>

                    <li>
                        If <m>B_1, B_2, \dots, B_\ell</m> are cycles of generalized eigenvectors corresponding to a fixed <m>\lambda</m> whose initial vectors are linearly independent, then the concatenation <m>B_1*B_2*\cdots * B_\ell</m> is linearly independent.
                    </li>
                </ol>
            </p>
        </statement>


        <proof>
            <p>
                <ol>
                    <li>
                        <p>
                            Suppose 
                            <me>
                                B=((T-\lambda I)^{k-1}(v),\dots, (T-\lambda I)(v), v)
                            </me>
                            is a cycle of generalized eigenvectors of <m>T</m> that is a basis of <m>V</m>. Let <m>N=T-\lambda I</m>, and let 
                            <me>
                                v_j=(T-\lambda I)^j(v)=N^j(v)
                            </me>. 
                            (In other words, <m>v_j</m> is the <m>k-j</m>-th element of <m>B</m>.) Clearly we have 
                            <me>
                                N(v_j)=v_{j+1}
                            </me>
                            for all <m>0\leq j\leq k-2</m>. Since <m>B</m> is a cycle, we also have <m>N(v_{k-1})=\boldzero</m>. Thus, using  <xref ref="eq_matrixrep_formula"/>, we have 
                            <me>
                                [N]_B=\begin{bmatrix}
                                0\amp 1 \amp 0\amp \cdots \amp 1 \\
                                0\amp 0\amp 1 \\
                                \vdots \amp \amp \ddots \amp \ddots \\
                                \amp \amp \amp \amp 1 \\
                                0\amp \dots \amp \amp \amp 0
                                \end{bmatrix}
                            </me>.
                            Since the matrix representation operator <m>[\phantom{A}]_B</m> is linear, we have 
                            <md>
                                <mrow>[T]_B \amp =[N+\lambda I]_B</mrow>
                                <mrow> \amp =[N]_B+\lambda[I]_B</mrow>
                                <mrow> \amp = \begin{bmatrix}
                                \lambda\amp 1 \amp 0\amp \cdots \amp 1 \\
                                0\amp \lambda\amp 1 \\
                                \vdots \amp \amp \ddots \amp \ddots \amp \\
                                \amp \amp \amp \amp 1\\
                                0\amp \dots \amp \amp \amp \lambda
                                \end{bmatrix}</mrow>
                            </md>.
                            The reverse implication (that if <m>[T]_B</m> is a Jordan block, then <m>B</m> is a cycle of generalized eigenvectors), follows essentially from the exact same observations in reverse. 
                        </p>
                    </li>
                    <li>
                        <p>
                            This follows easily from (1) and the fact that <m>\Span B</m> is <m>T</m>-invariant for any cycle of generalized eigenvectors <m>B</m>. 
                        </p>
                    </li>
                    <li>
                        <p>
                            This follows from <xref ref="th_Tcyclic_sub"/>, where we showed that if <m>k</m> is the smallest positive integer such that <m>T^k(v)=\boldzero</m>, then <m>(v,T(v),\dots, T^{k-1}(v))</m> is linearly independent.
                        </p>
                    </li>
                    <li>
                        <p>
                            We prove the result by induction on the sum of the lengths 
                            <men xml:id="eq_cycles_gen_eigen">
                            n=\len B_1+\len B_2+\cdots +\len B_\ell
                            </men>,
                            the base case <m>n=1</m> being trivial. Assume now that the result is true for any collection cycles of generalized <m>\lambda</m>-eigenvectors whose lengths sum to <m>k&lt; n</m>, and suppose that we have cycles <m>B_1,B_2,\dots, B_\ell</m> whose initial vectors are linearly independent, and whose lengths satisfy <xref ref="eq_cycles_gen_eigen"/>. Let <m>N=T-\lambda I</m>, and for each <m>1\leq j\leq \ell</m> let 
                            <me>
                                B_j=(v_{1,j},v_{2,j},\dots, v_{n_j,j})
                            </me>.
                            By definition, this means that 
                            <me>
                                N(v_{i,j})=\begin{cases}
                                v_{i-1,j}\amp  \text{if } i\geq 2\\
                                \boldzero\amp \text{if } i=1
                                \end{cases}
                            </me>.
                            If <m>\len B_j=1</m> for all <m>j</m>, then clearly the concatenation is linearly independent. Otherwise, 
                            observe that the collection of <q>shortened</q> tuples 
                            <me>
                                \widetilde{B}_j=(v_{1,j},v_{2,j},\dots, v_{n_j-1,j})
                            </me>
                            obtained by deleting (<q>popping</q>) the last element of each <m>B_j</m> of length greater than 1 are cycles with the same linearly independent initial vectors. Since the sum of the lengths of these cycles is less than <m>n</m>, the induction hypothesis applies to the <m>\widetilde{B_j}</m>: <ie/>, the concatenation of their elements is linearly independent.  
                        </p>
                        <p> 
                            Now suppose we have a linear combination of the elements of <m>B_i</m> equal to <m>\boldzero</m>: 
                            <me>
                               \sum_{j=1}^\ell \sum_{i=1}^{n_j}a_{ij}v_{i,j}=\boldzero 
                            </me>.
                            Now apply the linear operator <m>N</m> to both sides of this equation: 
                            <md>
                                <mrow>\sum_{j=1}^\ell \sum_{i=1}^{n_j}a_{ij}N(v_{i,j})\amp =\boldzero  </mrow>
                                <mrow>\sum_{j=1}^\ell \sum_{i=2}^{n_j}a_{ij}v_{i-1,j}=\boldzero  \amp </mrow>
                            </md>.
                            Note that this is a linear combination of the elements of the shortened cycles <m>\widetilde{B_j}</m>. Since their concatenation is linearly independent, we conclude that all of the coefficients <m>a_{ij}=0</m> with <m>1\leq j\leq \ell</m>, <m>2\leq i\leq n_j</m> are equal to 0. Thus the original linear combination is of the form 
                            <me>
                                \sum_{j=1}^\ell a_{1,j}v_{1,j}=\boldzero
                            </me>.
                            By assumption the tuple of initial vectors <m>(v_{1,j})</m> is linearly independent. We conclude that <m>a_{1,j}=0</m> for all <m>j</m>, and thus that the tuple <m>(v_{i,j})</m> is linearly independent, as desired.

                        </p>
                    </li>
                </ol>
            </p>
        </proof>
    </theorem>

    <theorem xml:id="th_jordan_form">
        <title>Jordan form</title>

        <statement>
            <p>
                    Let <m>T\in \mathcal{L}(V)</m> where <m>V</m> is finite dimensional, and suppose all roots of the characteristic polynomial of <m>T</m> lie in <m>\F</m>.
                    <ol>
                        <li>
                            There is a Jordan basis <m>B</m> for <m>T</m>.
                        </li>

                        <li>
                            The corresponding Jordan form 
                            <me> 
                                A=[T]_B=\begin{bmatrix} J_1\amp \\ \amp J_2\amp \\ \amp    \amp\ddots \\ \amp     \amp     \amp J_r \end{bmatrix}
                            </me> is unique up to a permutation of the Jordan blocks <m>J_{i}</m>.
                        </li>
                    </ol>
            </p>
        </statement>


        <proof>
            <title>Proof of (1)</title>
    <p>
        We first prove that the existence of a Jordan basis. Let <m>V=\oplus_{i=1}^rG(\lambda_i, T)</m> be the decomposition of <m>V</m> into generalized eigenspaces, as per <xref ref="th_gen_eigenspace_decomp"/>. Observe that since each <m>G(\lambda_i, T)</m> is <m>T</m>-invariant, if we can find Jordan bases <m>B_1,B_2,\dots, B_r</m> for <m>T\vert_{G(\lambda_i, T)}</m>, then the concatenation
        <me>
            B=B_1*B_2*\cdots *B_r
        </me>
        is a Jordan basis for <m>T</m>. Thus it suffices to prove the result for linear operators whose characteristic polynomial splits as <m>(x-\lambda)^n</m>. 
     </p>       
    <p>
        We now prove by induction on <m>n=\dim V</m>, that if <m>T\in\mathcal{L}(V)</m> is a linear operator with characteristic polynomial <m>f(x)=(x-\lambda)^n</m>, <m>\lambda\in F</m>, then <m>T</m> has a Jordan basis. The base case <m>n=1</m> is trivial, since any operator on a <m>1</m>-dimensional space has a matrix representation of the form <m>[T]_B=[\lambda]\in F^{1,1}</m>, which is clearly a Jordan form. 
    </p>
    <p>
        Assume now the result applies for any operator on any <m>k</m>-dimensional vector space, <m>k^lt; n</m>. Given <m>V</m> with <m>\dim V=n</m> and <m>T\in \mathcal{L}(V)</m> with characteristic polynomial <m>f(x)=(x-\lambda)^n</m>, let <m>N=T-\lambda I</m>, and let <m>W=\im N</m>. Since <m>\lambda</m> is an eigenvalue of <m>T</m>, <m>N</m> is non-invertible, and thus <m>\im N\ne V</m>. It follows that <m>\dim \im N=k&lt; n</m>. Moreover, <m>\im N</m> is <m>N</m>- and thus <m>T</m>-invariant, and since the characteristic polynomial of <m>T\vert_{\im N}</m> divides <m>(x-\lambda)^n</m>, we see that <m>T\vert_\{\im N\}</m> has characteristic polynomial <m>(x-\lambda)^k</m>. Thus we may apply the induction hypothesis to conclude that <m>\im N</m> has a Jordan basis that is a concatenation of the cycles of generalized <m>\lambda</m>-eigenvectors <m>B_1,B_2,\dots, B_\ell</m>. Write each <m>B_j</m> as 
        <me>
            B_j=(v_{1,j},\dots, v_{n_j, j})
        </me>,
        and observe that by definition we have 
        <me>
                                N(v_{i,j})=\begin{cases}
                                v_{i-1,j}\amp  \text{if } i\geq 2\\
                                \boldzero\amp \text{if } i=1
                                \end{cases}
                            </me>
        for all <m>v_{i,j}</m>. Since the end vector <m>v_{n_j, j}</m> of each cycle <m>B_j</m> is an element of <m>\im N</m>, for all <m>1\leq j\leq \ell</m> we can find a vector <m>v_{n_{j}+1,j}</m> satisfying 
        <me>N(v_{n_{j}+1,j})=v_{n_j,j}</me>,
        yielding <q>lengthened</q> cycles of generalized <m>\lambda</m>-eigenvectors 
        <me>
            \widetilde{B_j}=(v_{1,j},\dots, v_{n_j, j}, v_{n_j+1, j}) 
        </me>
        for all <m>1\leq j\leq \ell</m>.  
    </p>
    <p>
        Lastly, since the concatenation of the original cycles <m>B_i</m> is linearly independent, in particular, the tuple of their initial vectors <m>(v_{1,j})</m> is linearly independent. Since furthermore these elements are elements of <m>\NS N</m> (<m>N(v_{1,j})=\boldzero</m>), we can extend this tuple to a basis 
        <men xml:id="eq_jordan_form_null">
            (v_{1,1},\dots, v_{1,\ell}, w_{\ell+1}, \dots, w_{t})
        </men>
        of <m>\NS N</m>. We claim that the concatenation <m>B</m> of the cycles 
        <me>
            \widetilde{B_1}, \widetilde{B_1}, \dots, \widetilde{B_1}, (w_{\ell+1}), (w_{\ell+2}), \dots, (w_t)
        </me>
        is our desired Jordan basis of <m>T</m>. Since the initial vectors of these cycles is precisely the linearly independent tuple in <xref ref="eq_jordan_form_null"/>, it follows from <xref ref="th_jordan_cycles"/> that <m>B</m> is linearly independent. To prove <m>B</m> is a basis it suffices to show that <m>\len B=\dim V=n</m>. Let <m>B'=B_1*B_2*\cdots *B_\ell</m>, the starting Jordan basis of <m>\im N</m>.  We created <m>B</m> by adding one extra vector for each of the <m>\ell</m> cycles <m>B_j</m>, as well as the <m>r-\ell</m> vectors <m>w_j</m> added to get a basis of the <m>r</m>-dimensional subspace <m>\NS N</m>. Thus we have 
        <md>
            <mrow>\len B \amp = \len B'+\ell+r-\ell </mrow>
            <mrow> \amp =\dim \im N+r  (\dim \im N=\len B')</mrow>
            <mrow> \amp =\dim \im N+\dim \NS N</mrow>
            <mrow> \amp = n \amp (\text{rank-nullity})</mrow>
        </md>,
        as desired.
    </p>
    </proof>
    <proof>
        <title>Proof of (2)</title>
        <p>
            The uniqueness of the Jordan form follows from the fact that for each generalized eigenspace <m>W_\lambda=E(\lambda, T)</m> the number <m>\ell</m> of Jordan blocks for <m>T\vert_{W_\lambda}</m> as well as the list of lengths of these sequences can be expressed solely in terms of <m>\dim \NS (T-\lambda I)^j</m> for various <m>j\in \Z_+</m>, and hence depend solely on <m>T</m> itself and not on the particular manner of choosing a Jordan basis. Let's see why this is so. 
        </p>
        <p>
            Assume <m>T\in \mathcal{L}(V)</m> has characteristic polynomial <m>f(x)=(x-\lambda)^n</m>, let <m>N=T-\lambda I</m>, and suppose <m>B=B_1*B_2*\cdots *B_\ell</m> is a Jordan basis of <m>T</m>. Without loss of generality, we may assume that <m>\len B_j\geq \len B_{j+1}</m> for all <m>1\leq j\leq \ell</m>. Since the <m>B_j</m> are cycles of generalized <m>\lambda</m>-eigenvectors, we have 
            <me>
                B_j=(N^{n_j-1}(v_j),\dots, N(v_j),v_j)
            </me>,
            where <m>N^{n_j}(v_j)=\boldzero</m> for all <m>1\leq j</m>. The cycle structure allows us to easily choose subtuples of the Jordan basis <m>B=(N^{i}(v_j))</m> that serve as bases of <m>\dim \NS N^k</m> for various <m>k\in \Z_+</m>. Consider <m>\NS N</m> first. We claim that the tuple 
            <men xml:id="eq_jordan_unique_null">
                (N^{n_1-1}(v_1),N^{n_2-1}(v_2),\dots, N^{n_\ell-1}(v_\ell))
            </men>
            consisting of initial vectors of the <m>B_j</m> is a basis of <m>\NS N</m>. Indeed, given any <m>v\in V</m>, since <m>B=(N^i(v_j))</m> is a basis of <m>V</m>, we can write 
            <me>
                v=\sum_{j=1}^\ell\sum_{i=0}^{n_j-1} a_{ij}N^{i}(v_j)
            </me>
            for some <m>a_{ij}\in F</m>, whence 
            <md>
                <mrow>v\in \NS N \iff \amp \sum_{j=1}^\ell\sum_{i=0}^{n_j-1}a_{i,j}N(N^{i}(v_j))=\boldzero</mrow>
                <mrow>\amp \iff \sum_{j=1}^\ell\sum_{i=0}^{n_j-1}a_{i,j}N^{i+1}(v_j)=\boldzero \amp </mrow>
                <mrow> \amp \iff \sum_{j=1}^\ell\sum_{i=0}^{n_j-2}a_{i,j}N^{i+1}(v_j)=\boldzero \amp (N^{n_j}(v_j)=\boldzero)</mrow>
                <mrow> \amp \iff a_{i,j}=0 \text{ for } 1\leq j\leq \ell, 1\leq i\leq n_j-2</mrow>
            </md>,
            where the last line follows from linear independence. Thus <m>v\in \NS N</m> if and only if <m>v=\sum_{j=1}^\ell a_{n_j-1, j}N^{n_{j}-1}(v_j)</m>, showing that the tuple in <xref ref="eq_jordan_unique_null"/> spans <m>\NS N</m>. Since that tuple is linearly independent, we conclude it is a basis, as claimed.
        </p>
        <p>
            A similar argument, replacing <m>N</m> with <m>N^k</m> in our computations above, shows that the tuple 
            <me>
                (N^{i}(v_j))_{\substack{1\leq j\leq \ell\\ n_j-k+1\leq i\leq n_j}}
            </me>
            is a basis of <m>\NS N^k</m>. 
            The idea behind these arguments is nicely visualized by arranging the cycles <m>B_j</m> in vertical columns as follows:
            <me>
                \begin{array}{cccc}
                B_1 \amp B_2\amp \dots \amp B_\ell\\
                \hline
                N^{n_1-1}(v_1) \amp N^{n_2-1}(v_2) \amp \dots \amp N^{n_\ell-1}(v_\ell)\\
                N^{n_1-2}(v_1) \amp N^{n_2-2}(v_2) \amp  \amp \vdots \\
                N^{n_1-3}(v_1) \amp \vdots \amp  \amp v_\ell  \\
                \vdots \amp v_2\amp \amp \\
                v_1 \amp  \amp \amp 
                \end{array}
            </me>.
            Applying <m>N</m> to the basis <m>B</m> moves vectors in the top row to <m>\boldzero</m> and moves all other vectors up one row. Moreover, from our discussion above, we see that the top <m>k</m> rows of the array form a basis for <m>\NS N^{k}</m>, and thus that the number <m>r_k</m> of vectors in the <m>k</m>-th row of the array is given by 
            <men xml:id="eq_jordan_rows">
                \dim \NS N^k-\dim \NS N^{k-1}
            </men>,
            for all <m>1\leq k\leq n</m>. This means the precise shape of the array is determined by the values 
            <xref ref="eq_jordan_rows"/>, which depend only on <m>T</m>. Knowing the precise shape of this array tells us in turn the essential details of the Jordan block decomposition. For example, we have 
            <me>
                \dim \NS N=r_1=\ell=\# (\text{cycles }B_j)=\#(\text{Jordan blocks})
            </me>.
            Similarly, we can deduce the length of each column in our array, and hence <m>\len B_j</m> (equivalently, the size of the <m>j</m>-th Jordan block), from the sequence <m>r_1,r_2,\dots, r_n</m>. Indeed, we have 
            <me>
                \len B_j=\max\{k \colon r_k\geq j\}
            </me>,
            as one easily verifies. 
            For example, if the sequence is 
            <me>
                r_1=5, r_2=4, r_3=2, r_4=2, r_5=0
            </me>,
            then we deduce that 
            <me>\len B_1=4, \len B_2=4, \len B_3=2, \len B_4=2, \len B_5=1</me>.
        </p>
    </proof>
    </theorem>
<definition xml:id="d_Jordan_form">
<title>Jordan form</title>
<statement>
<p>
Let <m>T,S\in \mathcal{L}(V)</m>, where <m>V</m> is finite dimensional. Assume that the characteristic polynomials of <m>T</m> and <m>S</m> both split completely over the base field. We say <m>T</m> and <m>S</m> have the <term>same Jordan form</term> if they have Jordan forms that differ by a permutation of the Jordan blocks. 
</p>
</statement>
</definition>

   

    <algorithm xml:id="proc_dot_diagram">
        <title>Dot diagram</title>
        <statement>
            <p>
              Let <m>T\in \mathcal{L}(V)</m>, where <m>V</m> is finite dimensional.
Fix an eigenvalue <m>\lambda</m>, and let <m>B=B_{1}*B_{2}*\dots * B_{r}</m> be a Jordan basis of <m>G(\lambda, T)</m>, where each <m>B_{j}</m> is a cycle of generalized <m>\lambda</m>-eigenvectors satisfying <m>\len B_j\geq \len B_{j+1}</m> for all <m>1\leq j\leq r-1</m>.
The <term>dot diagram</term> of <m>B</m> is an staircase pattern of dots constructed in the manner described below. 

For all <m>i\geq 1</m>, let <m>r_{i}</m> be the number of dots in the <m>i</m>-th row of the dot diagram.
<ul>
    <li>
        <p>
           Dots are placed into a rectangular grid containing <m>\len B_1</m> rows and <m>r</m> columns. For all <m>1\leq i\leq p</m>, the <m>i</m>-th row contains <m>r_{i}</m> dots, and these are placed consecutively starting in the leftmost column.  
        </p>
    </li>
    <li>
        <p>
            We have 
            <me>
                r_i=\dim \NS(T-\lambda I)^{i}-\dim\NS(T-\lambda I)^{i-1}.
            </me>
            for all <m>1\leq i</m>. 
        </p>
    </li>
</ul>
The dot diagram is interpreted as follows. 
<ul>
    <li>
        <p>
            For any <m>k\geq 1</m>, the total number of dots in the first <m>i</m> rows of the diagram is equal to <m>\dim \NS (T-\lambda I)^{k}</m>: <ie/>, 
            <me>
                \dim \NS (T-\lambda I)^k=\sum_{i=1}^kr_i
            </me>.
            In particular, we have 
            <me>
                r=r_1=\dim \NS(T-\lambda I)
            </me>.
            Thus the number of <m>\lambda</m>-Jordan blocks of <m>T</m> is equal to the number of dots in the first row of the diagram, which is equal to  <m>\dim \NS(T-\lambda I)</m>.
        </p>
    </li>
    <li>
        <p>
            For each <m>1\leq j\leq r</m>, the number of dots <m>c_j</m> in the <m>j</m>-th column of the diagram is equal to the length of <m>B_j</m>. Equivalently, <m>c_j</m> is the size of the <m>j</m>-th <m>\lambda</m>-Jordan block. 
        </p>
    </li>
    

    
</ul>
            </p>
        </statement>
    </algorithm>
<p>
We illustrate how to compute a normal form for a matrix transformation <m>T_A\colon F^n\rightarrow F^n</m> in <xref first="eg_jordan_form_1" last="eg_jordan_form"/>. Since those examples take up quite a lot of space, we place them at the end of the section. In the meantime, we list some interesting theoretical consequences of <xref ref="th_jordan_form"/>.  

</p>
<theorem xml:id="th_min_poly_Jordan">
<title>Minimal polynomial from Jordan form</title>
<statement>
<p>
Let <m>T\in \mathcal{L}(V)</m>, where <m>V</m> is finite dimensional, and suppose the characteristic polynomial <m>f</m> of <m>T</m> factors as 
<me>
    f(x)=\prod_{i=1}^r(x-\lambda_i)^{n_i}
</me>,
where <m>\lambda_i\ne \lambda_j</m> for <m>i\ne j</m>, and <m>\lambda_i\in F</m>. Let <m>m_T(x)</m> be the minimal polynomial of <m>T</m>. We have 
<me>
    m_T(x)=\prod_{i=1}^r(x-\lambda_i)^{m_i}
</me>,
where <m>m_i</m> is the maximum size of a  <m>\lambda_i</m>-Jordan block appearing in a Jordan form for <m>T</m>. 
</p>
</statement>
<proof>
<p>
    Let <m>V=\bigoplus_{i=1}^{r}G(\lambda_i, T)</m>, and for each <m>1\leq i\leq r</m>, let <m>T_i=T\vert_{G(\lambda_i, T)}</m> and let <m>m_i(x)</m> be the minimal polynomial of <m>T_i</m>. Since <m>m_T(T)=0_V</m>, we have 
    <me>
        m_i(T)=m_i(T_i)=0_{G(\lambda_i,T)}
    </me>
    for all <m>i</m>, and hence <m>m_i\mid m</m> for all <m>i</m>; since further, we have <m>m_i(x)=(x-\lambda_i)^{m_i}</m> for some <m>m_i\in \Z</m> with <m>\lambda_i\ne \lambda_j</m>, it follows that 
    <me>
        \prod_{i=1}^{r}m_i(x)\mid m_T(x)
    </me>.
    Lastly, using a decomposition <m>v=w_1+w_2+\cdots +w_r</m>, with <m>w_r\in G(\lambda_i, T)</m>, we see easily that 
    <me>
       \prod_{i=1}^{r}m_i(T)v=\sum_{i=1}^{r}\prod_{i=1}^{r}m_i(T)w_i=\boldzero 
    </me>,
    and hence that <m>m_T(x)\mid \prod_{i=1}^{r}m_i(x)</m>. Since <m>m_T(x)</m> and <m>\prod_{i=1}^{r}m_i(x)</m> are both monic, and are each a factor of the other, it follows that <m>m_T(x)=\prod_{i=1}^{r}m_i(x)</m>. In other words, the minimal polynomial of <m>T</m> is the product of the minimal polynomials of the restrictions <m>T\vert_{G(\lambda_i, T)}</m>.
</p>
<p>
  Since we compute a Jordan form of <m>T</m> be computing Jordan forms for each restriction <m>T\vert_{G(\lambda_i, T)}</m>, it now suffices to prove the claim for <m>T\vert_{G(\lambda_i, T)}</m>. Thus without loss of generality, we may assume that <m>T</m> has characteristic polynomial <m>f(x)=(x-\lambda)^n</m>. Let <m>B</m> be a Jordan basis of <m>T</m>, and let 
  <me>
    A=[T]_B=\begin{bmatrix}
    J_1\amp 0 \amp \dots \amp \amp 0  \\
    0\amp J_2\amp 0 \\
    \vdots \amp \amp \vdots \amp  \\
    0\amp \dots \amp \amp \amp J_\ell
    \end{bmatrix}
  </me>
  be the corresponding Jordan form of <m>T</m>,
  where each <m>J_i</m> is a <m>\lambda</m>-Jordan block of dimension <m>k_i\times k_i</m>. Since each <m>J_i</m> has characteristic polynomial <m>(x-\lambda)^{k_i}</m>, we see that the minimal polynomial of <m>J_i</m> is of the form <m>(x-\lambda)^{j_i}</m> for some <m>j_i\leq k_i</m>. An easy proof by induction now shows that 
  <me>
    (J_i-\lambda I)^\ell=\boldzero \iff \ell \geq k_i
  </me>. 
  In other words, in general the minimal polynomial of a <m>\lambda</m>-Jordan block of dimension <m>k\times k</m> is <m>(x-\lambda)^k</m>, as is easily shown. Lastly, from the block diagonal description of <m>A=[T]_B</m>, it follows that 
  <me>
    g(A)=\begin{bmatrix}
    g(J_1)\amp 0 \amp \dots \amp 0  \\
    0\amp g(J_2)\amp 0 \\
    \vdots \amp \amp \vdots \amp  \\
    0\amp \dots \amp \amp \amp g(J_k)
    \end{bmatrix}
  </me>,
  for any polynomial <m>g\in P(F)</m>. We conclude that 
  <md>
    <mrow>g(A)=\boldzero \amp \iff g(J_i)=\boldzero \text{ for all } 1\leq i\leq \ell</mrow>
    <mrow> \amp \iff (x-\lambda)^{k_j}\mid g(x) \text{ for all } 1\leq i\leq \ell</mrow>
    <mrow> \amp \iff (x-\lambda)^m\mid g(x) \text{ where } m=\max\{k_j\}</mrow>
  </md>.
  It follows that the minimal polynomial of <m>A</m>, and hence of <m>T</m> is <m>(x-\lambda)^m</m>, where <m>m</m> is the maximum size of the Jordan blocks <m>J_i</m>, as desired. 
</p>
</proof>
</theorem>
     <theorem xml:id="th_similarity">
        <title>Similarity</title>
        <statement>
            <p>
               Let <m>A, A'\in \F^{n,n}</m> and suppose both characteristic polynomials factor completely over <m>\F</m>.
The following statements are equivalent.
<ol>
    <li>
        <p>
            <m>A</m> and <m>A'</m> are similar (i.e. <m>A'=P^{-1}AP</m> for some invertible <m>P</m>).
        </p>
    </li>

    <li>
        <p>
            <m>T_A</m> and <m>T_A'</m> have the same Jordan form.
        </p>
    </li>
</ol>
            </p>
        </statement>
    <proof>
        <p>
            Assume <m>A</m> and <m>A'</m> are similar, and thus that there is an invertible matrix <m>Q</m> such that <m>A=Q^{-1}A'Q</m>. Let <m>J</m> be a Jordan form for <m>T_A</m>. By definition, there is a basis <m>B'</m> such that 
            <me>
                [T_A]_{B'}=J
            </me>.
            Let <m>B</m> be the standard basis, and let <m>P=\underset{B'\to B}{P}</m>. Using the change of basis formula, we have 
            <me>
                J=P^{-1}AP
            </me>,
            and thus 
            <me>
                J=P^{-1}Q^{-1}A'QP=(QP)^{-1}A'(QP)
            </me>.
            Equivalently, letting <m>B''</m> be the basis consisting of the columns of <m>QP</m>, we see that 
            <me>
                J=[T_{A'}]_{B''}
            </me>.
            Thus <m>A</m> and <m>A'</m> have the same Jordan form. 
        </p>
        <p>
            Now assume <m>A</m> and <m>A'</m> have the same Jordan form <m>J</m>. Reasoning as above, this implies that there are invertible matrices <m>P</m> and <m>Q</m> such that 
            <me>
                J=P^{-1}AP=Q^{-1}A'Q
            </me>.
            It follows that 
            <me>
                A=PQ^{-1}A'QP^{-1}=(QP^{-1})^{-1}A'(QP^{-1})
            </me>,
            showing that <m>A</m> and <m>A'</m> are similar.
        </p>
    </proof>
    </theorem>
    <theorem xml:id="th_diagable_plus_nilpotent">
    <title>Diagonal plus nilpotent</title>
    <statement>
    <p>
    Let <m>T\in \mathcal{L}(V)</m>, where <m>V</m> is finite-dimensional, and assume the characteristic polynomial of <m>T</m> splits completely over the base field. There are operators <m>S,N\in \mathcal{L}(V)</m> satisfying the following conditions: 
    <ul>
        <li>
            <p>
                <m>T=S+N</m>;
            </p>
        </li>
        <li>
            <p>
                <m>S</m> is diagonalizable; 
            </p>
        </li>
        <li>
            <p>
                <m>N</m> is nilpotent (<ie/>, <m>N^k=0_V</m> for some <m>k\in \Z_+</m>).
            </p>
        </li>
    </ul>
    In other words <m>T</m> is the sum of a diagonalizable and a nilpotent operator. 
    </p>
    </statement>
    <proof>
    <p>
        Let <m>B</m> be a Jordan basis of <m>T</m>, and let 
        <me>
            J=[T]_B=\begin{bmatrix}J_{1}\amp \\ \amp J_{2}\amp \\ \amp \amp \ddots \\ \amp \amp \amp J_{\ell} \end{bmatrix}
        </me>
        be its corresponding Jordan form. It is not difficult to see that we can write 
        <me>
            J=D+M
        </me>,
        where <m>D</m> is diagonal and <m>M</m> is upper triangular with zeros along the diagonal. Moreover, this implies that <m>M^k=\boldzero</m> for some <m>k\in \Z</m>: <ie/>, any such <m>M</m> is nilpotent. Since the matrix representation operator <m>[\phantom{A}]_B</m> is an isomorphism of <m>\mathcal{L}(V)</m> to <m>F^{n,n}</m>, <m>n=\dim V</m>, there are operators <m>S, N\in \mathcal{L}(V)</m> satisfying 
        <me>
            [S]_B=D, [N]_B=M
        </me>.
        Furthermore, since <m>D</m> is diagonal and <m>M</m> is a nilpotent matrix,  it follows that <m>S</m> is diagonalizable and <m>N</m> is a nilpotent operator. Lastly, since 
        <me>
            [S+N]_B=[S]_B+[N]_B=D+M=J=[T]_B
        </me>,
        and since <m>[\phantom{A}]_B</m> is bijective, it follows that <m>T=S+N</m>. 
    </p>
    </proof>
    </theorem>
    
    <theorem xml:id="th_square_roots">
    <title>Square-roots of complex matrices</title>
    <statement>
    <p>
    Let <m>A\in \C^{n,n}</m> be an invertible matrix. There is a matrix <m>B</m> such that <m>B^2=A</m>. In other words, every invertible complex matrix has a square root. 
    </p>
    <p>
        It follows that any invertible operator on a finite-dimensional complex vector space (<m>F=\C</m>) has a square-root. 
    </p>
    </statement>
    <proof>
    <p>
        From calculus we learn that the infinite series 
        <me>
            \sum_{i=0}^\infty {1/2\choose i}x^i=1+\frac{1}{2}x-\frac{1}{8}x^2+\frac{1}{16}x^3-\cdots 
        </me>
        converges to <m>\sqrt{1+x}</m> for all <m>x\in (-1,1)</m>. It follows from power series theory that for all <m>N</m>, the truncated series 
        <me>
            f_m(x)=\sum_{i=0}^m{1/2\choose i}x^i
        </me>
        satisfies 
        <me>
            f_m(x)f_m(x)=1+x+x^{m+1}h_m(x)
        </me>
        for some polynomial <m>h_m(x)</m>. Thus if <m>A=I+N</m> where <m>N</m> is a nilpotent matrix satisfying 
        <me>
            N^{m+1}=\boldzero
        </me>,
        then 
        <me>
            f_m(N)f_m(N)=I+N+N^{m+1}h_m(N)=I+N,
        </me>
        since <m>N^{m+1}=\boldzero</m>. This proves that any matrix of the form <m>I+N</m> has a square root <m>B</m>, and that in fact we may take <m>B=f_m(N)</m> for <m>m</m> sufficiently large. 
    </p>
    <p>
        Now assume <m>A</m> is invertible. From <xref ref="th_diagable_plus_nilpotent"/> and Jordan theory, we can write <m>A=D+N</m>, where <m>D</m> is diagonal and <m>N</m> is nilpotent, and furthermore the diagonal entries of <m>D</m> are the eigenvalues of <m>A</m>, which are all nonzero. It follows that <m>D</m> is diagonal. Write <m>A=D(I+D^{-1}N)</m>. Since <m>D^{-1}</m> is diagonal, it commutes with all matrices, and it follows that <m>D^{-1}N</m> is also nilpotent. By the previous paragraph there is a square-root <m>B</m> of <m>I+D^{-1}N</m>. Let <m>E</m> be a diagonal square-root of <m>D</m> (it is easy to see such a matrix exists), and let <m>C=EB</m>. We have 
        <md>
            <mrow>C^2 \amp =EBEB</mrow>
            <mrow> \amp =EEBB \amp (E \text{ diagonal})</mrow>
            <mrow> \amp =E^2B^2</mrow>
            <mrow> \amp =D(I+D^{-1}N)</mrow>
            <mrow> \amp =A</mrow>
        </md>.
    </p>
    </proof>
    </theorem>
<p>
    At last we are ready to do some computations around Jordan forms and bases. In order to illustrate the subtleties of this analysis we need to work with larger matrices than usual, and this in turn compels us to reach for some computing technology. The wonderful open source computer algebra system Sage serves our needs for this task. 
</p>
<example xml:id="eg_jordan_form_1">
<title>Computing Jordan form</title>
<statement>
<p>
Consider the matrix transformation <m>T=T_A\colon \R^8\rightarrow \R^8</m>, where 
<me>
    A=\begin{amatrix}[rrrrrrrr]
3 \amp 2 \amp 3 \amp 2 \amp 3 \amp -1 \amp -3 \amp -3 \\
-5 \amp -4 \amp -13 \amp -15 \amp -15 \amp 2 \amp 16 \amp 1 \\
-1 \amp -1 \amp -5 \amp -7 \amp -6 \amp 1 \amp 7 \amp -2 \\
-1 \amp -2 \amp 7 \amp 23 \amp 11 \amp -5 \amp -20 \amp 21 \\
0 \amp 0 \amp 2 \amp -7 \amp -1 \amp 1 \amp 7 \amp -8 \\
1 \amp 1 \amp 3 \amp 1 \amp 2 \amp 0 \amp -1 \amp -2 \\
-2 \amp -3 \amp 2 \amp 7 \amp 2 \amp -3 \amp -4 \amp 11 \\
-1 \amp -1 \amp -5 \amp -14 \amp -9 \amp 2 \amp 14 \amp -8
    \end{amatrix}
</me>.
Compute the characteristic polynomial <m>f</m> of <m>A</m>. 
</p>
<sage>
    <input>
        A=matrix([(3, 2, 3, 2, 3, -1, -3, -3), (-5, -4, -13, -15, -15, 2, 16, 1), (-1, -1, -5, -7, -6, 1, 7, -2), (-1, -2, 7, 23, 11, -5, -20, 21), (0, 0, 2, -7, -1, 1, 7, -8), (1, 1, 3, 1, 2, 0, -1, -2), (-2, -3, 2, 7, 2, -3, -4, 11), (-1, -1, -5, -14, -9, 2, 14, -8)])
A.characteristic_polynomial().factor()
    </input>
    <output>
        
    </output>
</sage>
<p>
We see that <m>f(x)=(x-2)^4(x+1)^4</m>. This tells us that <m>V=\R^8</m> decomposes into two generalized eigenspaces 
<me>
    V=G(2,T)\oplus G(-1,T)
</me>,
and moreover (looking at the multiplicities), that <me>\dim G(2,T)=\dim G(-1,T)=2</me>. 
</p>
<p>
    Now compute the dot diagram for <m>\lambda=2</m>. Since <m>\dim(2,T)=4</m>, we know that <m>G(2, T)=\NS(T-2I)^4</m>, and thus we only need to look at <m>\dim \NS (T-2I)^i</m> for <m>1\leq i\leq 4</m>. 
</p>
<sage>
    <input>
        N=A-2
        for i in range(1,5):
            print((N^i).nullity())
    </input>
    <output>
    </output>
</sage>
<p>
    Let <m>r_i</m> be the number of dots in the <m>i</m>-th row of our dot diagram for <m>\lambda=2</m>. From the computation above, we see that 
    <md>
        <mrow>r_1 \amp =\dim \NS(A-2I)=3</mrow>
        <mrow>r_2 \amp =\dim \NS(A-2I)^2-\dim \NS(A-2I)=1</mrow>
        <mrow>r_i \amp =\dim \NS(A-2I)^i-\dim\NS(A-2I)^{i-1}=0, i\geq 3</mrow>
    </md>.
    The dot diagram for <m>\lambda=2</m> is thus 
    <me>{\Huge
        \begin{array}{rrr}
        \cdot \amp \cdot \amp \cdot \\
        \cdot \amp \amp 
        \end{array}}
    </me>
    and the Jordan form of <m>T\vert_{G(2, T)}</m> breaks into 3 blocks of size <m>2\times 2</m>, <m>1\times 1</m>, and <m>1\times 1</m>. 
    <me>
        \left[\begin{array}{rr|r|r}
        2\amp 1 \amp 0 \amp 0\\
        0\amp 2\amp 0\amp 0 \\
        \hline
        0\amp 0\amp 2\amp 0 \\
        \hline
        0\amp 0\amp 0\amp 2
        \end{array}
        \right]
    </me>.
</p>
<p>
    Now compute the dot diagram for <m>\lambda=-1</m>.
</p>
<sage>
    <input>
        N=A+1
        for i in range(1,5):
            print((N^i).nullity())
    </input>
    <output>
        
    </output>
</sage>

<p>
    The corresponding dot diagram now consists of a single column
    <me>
        {\Large
        \begin{array}{r}
        \cdot\\
        \cdot\\
        \cdot\\
        \cdot
        \end{array}
        }
    </me>
    and thus the Jordan form for <m>T\vert_{G(-1,T)}</m> consists of a single <m>4\times 4</m> Jordan block
    <me>
        \begin{bmatrix}
        -1\amp 1\amp 0\amp 0\\
        0\amp -1\amp 1\amp 0\\
        0\amp 0\amp -1\amp 1\\
        0\amp 0\amp 0\amp -1
        \end{bmatrix}
    </me>.
    In summary, we conclude that there is a Jordan basis <m>B</m> of the form 
    <me>
        B=B_1*B_2*B_3*B_4
    </me>, 
    where 
    <md>
        <mrow>B_1 \amp =((T-2I)v_1,v_1)</mrow>
        <mrow>B_2 \amp =(v_2)</mrow>
        <mrow>B_2 \amp =(v_3)</mrow>
        <mrow>B_4 \amp =((T+I)^3(v_4),(T+I)^2(v_4),(T+I)(v_4),v_4)</mrow>
    </md>
    for some nonzero vectors <m>v_1,v_2,v_3,v_4</m>, and we have 
    <me>
        [T]_B=\left[\begin{array}{rr|r|r|rrrr}
2 \amp 1 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 2 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\hline
 0 \amp 0 \amp 2 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\hline
 0 \amp 0 \amp 0 \amp 2 \amp 0 \amp 0 \amp 0 \amp 0 \\
\hline
 0 \amp 0 \amp 0 \amp 0 \amp -1 \amp 1 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \amp -1 \amp 1 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp -1 \amp 1 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp -1
\end{array}\right]
    </me>.
Let us confirm with Sage.   
</p>
<sage>
    <input>
        show(A.jordan_form())
    </input>
    <output>
        
    </output>
</sage>

</statement>

</example>
<example xml:id="eg_jordan_form">
<title>Computing Jordan form</title>
<statement>
<p>
    Let <m>T=T_A\colon \R^8\rightarrow \R^8</m>, where 
    <me>
        A=\begin{amatrix}[rrrrrrrr]
        -\frac{3}{2} \amp 0 \amp 0 \amp -1 \amp 0 \amp -1 \amp 1 \amp -3 \\
3 \amp \frac{5}{2} \amp -4 \amp 4 \amp -1 \amp -2 \amp 4 \amp 1 \\
-3 \amp -2 \amp \frac{9}{2} \amp -4 \amp 1 \amp 1 \amp -6 \amp -3 \\
-3 \amp -12 \amp 8 \amp -\frac{7}{2} \amp -3 \amp 8 \amp -18 \amp 5 \\
-2 \amp -8 \amp 6 \amp -3 \amp -\frac{3}{2} \amp 6 \amp -12 \amp 4 \\
1 \amp -9 \amp 2 \amp 2 \amp -5 \amp \frac{13}{2} \amp -9 \amp 8 \\
-1 \amp 0 \amp 2 \amp -2 \amp 1 \amp 1 \amp -\frac{1}{2} \amp 0 \\
0 \amp 4 \amp -2 \amp 0 \amp 2 \amp -3 \amp 5 \amp -\frac{5}{2}
        \end{amatrix}
    </me>.
    <ol>
        <li>
            <p>
                Use the SageCells below to compute a Jordan normal form of <m>T</m> in a step-by-step fashion, as in <xref ref="eg_jordan_form_1"/>. Confirm your answer using the <c>jordan_form()</c> command. 
            </p>
            <sage>
                <input>
                    A=matrix([(-3/2, 0, 0, -1, 0, -1, 1, -3), (3, 5/2, -4, 4, -1, -2, 4, 1), (-3, -2, 9/2, -4, 1, 1, -6, -3), (-3, -12, 8, -7/2, -3, 8, -18, 5), (-2, -8, 6, -3, -3/2, 6, -12, 4), (1, -9, 2, 2, -5, 13/2, -9, 8), (-1, 0, 2, -2, 1, 1, -1/2, 0), (0, 4, -2, 0, 2, -3, 5, -5/2)])
                    show(A)
                    # compute the characteristic polynomial of A and factor
                </input>
                <output>
                    
                </output>
            </sage>
            <sage>
                <input>
                    # compute the dot diagrams for each eigenvalue of A
                </input>
                <output>
                    
                </output>
            </sage>
            <sage>
                <input>
                    # Verify your answer with jordan_form()
                </input>
                <output>
                    
                </output>
            </sage>
        </li>
        <li>
            <p>
                Having computed the Jordan normal form for <m>T</m>, hazard a guess as to what happens to <m>T^k</m> as we let <m>k\to \infty</m>. Verify or falsify your guess in the SageCell below. 
            </p>
            <sage>
                <input>
                    # Compute A^k and see what happens as k goes to infty
                    # Tip N(A) gives decimal versions of the entries of a matrix.
                </input>
                <output>
                    
                </output>
            </sage>
        </li>

    </ol>
    
</p>
</statement>

</example>
<p>
    Having successfully computed a few Jordan <em>forms</em> for linear operators, we should point out that we have <em>not</em> yet computed an actual Jordan <em>basis</em>. An algorithm for computing Jordan bases can be extracted from the proof of <xref ref="th_jordan_form"/>, though it requires significantly more computational work. We content ourselves with illustrating the method with a manageable example, done systematically.
</p>
<example xml:id="eg_jordan_basis">
<title>Jordan basis</title>
<statement>
<p>
Let <m>T=T_A\colon \R^6\rightarrow \R^6</m>, where
<me>
    A=\left[\begin{array}{rrrrrr}
2 \amp -1 \amp 1 \amp 2 \amp -1 \amp 2 \\
-3 \amp 3 \amp -4 \amp -8 \amp 10 \amp -14 \\
-2 \amp 1 \amp -1 \amp -7 \amp 8 \amp -12 \\
0 \amp 0 \amp 0 \amp 1 \amp 1 \amp -2 \\
-2 \amp 0 \amp -2 \amp -5 \amp 9 \amp -10 \\
-1 \amp 0 \amp -1 \amp -2 \amp 3 \amp -2
\end{array}\right]
</me>.
Proceeding as in the examples above, we find that the characteristic polynomial of <m>T</m> is <m>f(x)=(x-2)^6</m>, and that a normal form of <m>T</m> is 
<me>
    \left[\begin{array}{rrr|rr|r}
2 \amp 1 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 2 \amp 1 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 2 \amp 0 \amp 0 \amp 0 \\
\hline
 0 \amp 0 \amp 0 \amp 2 \amp 1 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 2 \amp 0 \\
\hline
 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 2
\end{array}\right]
</me>.
A Jordan basis of <m>T</m> will thus consist of three cycles 
<md>
    <mrow>B_1 \amp =(N^2(v_1),N(v_1),v_1)</mrow>
    <mrow>B_2 \amp =(N(v_2),v_2)</mrow>
    <mrow>B_3 \amp =(v_3)</mrow>
</md>.
It is illuminating to arrange this basis as in our dot diagrams: 
<me>
    \begin{array}{ccc}
    N^2(v_1) \amp N(v_2) \amp v_3\\
    N(v_1) \amp v_2 \amp \\
    v_1 \amp \amp
    \end{array}
</me>
Following the argument in <xref ref="th_jordan_form"/>, we will first compute a Jordan basis for <m>\im N^2</m>, then modify this appropriately to get a Jordan basis of <m>\im N</m>, then repeat the previous step to get a Jordan basis of <m>\im T^0=\R^6</m>.  
</p>
<p>
    From the diagram above, we see that <m>\dim \im N^2=1</m>, and that <m>\im N^2\subseteq \NS N=E(2, T)</m>. Thus any basis of <m>\im N^2=\CS (A-2I)^2</m> will consist of a single eigenvector, and thus will automatically be a Jordan basis. As we see below, we can choose <m>w_1=(1,-1,-1,0,0)</m> for the sole element of this basis.
</p>
<sage>
    <input>
        A=matrix([(2, -1, 1, 2, -1, 2), (-3, 3, -4, -8, 10, -14), (-2, 1, -1, -7, 8, -12), (0, 0, 0, 1, 1, -2), (-2, 0, -2, -5, 9, -10), (-1, 0, -1, -2, 3, -2)])
        N=A-2
        show(N)
        show(N^2)
    </input>
    <output>
        
    </output>
</sage>
<p>
    Following the induction argument in the proof of <xref ref="th_jordan_form"/>, to create a Jordan basis of <m>\im N^2</m>, we (a) lengthen the cycle <m>(w_1)</m> to a cycle <m>(T(u_1), u_1)</m>, where <m>u_1\in \im N</m> and <m>T(u_1)=w_1</m>, (b) extend <m>(w_1)</m> to a basis <m>(w_1,w_2)</m> of <m>\NS T\cap \im N</m>, producing the two cycles of generalized eigenvectors <m>(T(u_1),u_1), (w_2)</m>, which form a basis of <m>\im N</m>. From the computation of <m>N^2</m> above, we see that <m>N^2\bolde_1=w_1</m>. Thus we can pick 
    <me>
        u_1=N\bolde_1=(0,-3,-2,0,-2,-1)
    </me>.
    To extend <m>(w_1)</m> to a basis of <m>\NS N\cap \im N</m>, we first use Sage to compute a basis of this space. 
</p>
<sage>
    <input>
        W1=N.right_kernel()
        W2=N.column_space()
        W=W1.intersection(W2)
        W.basis()
    </input>
    <output>
        
    </output>
</sage>
<p>
    Since <m>\dim \NS N\cap \im N=2</m>, we see that we can add either of the basis elements above to <m>w_1</m> to get a basis. We choose <m>w_2=(1,5,4,1,5,2)</m>. A Jordan basis of <m>\im N</m> is then obtained by concatenating the two cyles of generalized eigenvectors below
    <md>
        <mrow>(N^2\bolde_1,N\bolde_1) \amp = ((1,-1,-1,0,0,0),(0,-3,-2,0,-2,-1)) </mrow>
        <mrow>(w_2) \amp = (1,5,4,1,5,2)</mrow>
    </md>.
    We now repeat this process to transform the Jordan basis of <m>\im N</m> into a Jordan basis of <m>\R^6</m>. First we lengthen the two cycles above. Clearly the first cycle can be lengthened by appending <m>\bolde_1</m>. For the second we find <m>v_2</m> satisfying <m>Nv_2=w_2</m>. As we show below, we can choose <m>v_2=(0,-3,0,-1,0,0)</m>. 
</p>
<sage>
    <input>
        N.solve_right(vector([1,5,4,1,5,2]))
    </input>
    <output>
        
    </output>
</sage>
<p>This yields the two cycles 
    <md>
        <mrow>(N^2\bolde_1, N\bolde_1, \bolde_1) \amp = ((1,-1,-1,0,0,0),(2,-3,-2,0,-2,1),(1,0,0,0,0,0)) </mrow>
        <mrow>(Nv_2,v_2) \amp = ((1,5,4,1,5,2),(0,-3,0,-1,0,0))</mrow>
    </md>.
    Lastly, we extend <m>(N^2\bolde_1, N\boldv_2)</m> to a basis <m>(N^2\bolde_1, N\boldv_2,v_3)</m> of <m>\NS N</m>. This can be done systematically by putting <m>N^2\bolde_1, N\boldv_2</m> in as the first two columns of a matrix, followed by a complete basis for <m>\NS N</m>, and then using the column space algorithm to contract to a basis. The computation below creates this augmented matrix; we see by inspection that we can pick <m>v_3=(1,1,0,1,1,0)</m>.
</p>
<sage>
    <input>
        A1=matrix([[1,-1,-1,0,0,0],[1,5,4,1,5,2]]).transpose()
        A2=N.right_kernel_matrix().transpose()
        A1.augment(A2)
    </input>
    <output>
        
    </output>
</sage>
<p>
    Putting it all together, we obtain the following cycles, whose concatenation is a Jordan basis for <m>T</m>: 
    <md>
        <mrow>(N^2(v_1),N(v_1),v_1)\amp= ((1,-1,-1,0,0,0),(0,-3,-2,0,-2,-1),(1,0,0,0,0,0)) </mrow>
        <mrow>(N(v_2),v_2) \amp= ((1,5,4,1,5,2), (0,-3,0,-1,0,0)) </mrow>
        <mrow> (v_3) \amp=(1,1,0,1,1,0) </mrow>
    </md>
    Time to put our theory to the test. We should obtain the Jordan form of <m>T</m> by the change of basis formula 
    <me>
        P^{-1}AP
    </me>,
    where 
    <me>
        P=\left[\begin{array}{rrrrrr}
1 \amp 0 \amp 1 \amp 1 \amp 0 \amp 1 \\
-1 \amp -3 \amp 0 \amp 5 \amp -3 \amp 1 \\
-1 \amp -2 \amp 0 \amp 4 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 1 \amp -1 \amp 1 \\
0 \amp -2 \amp 0 \amp 5 \amp 0 \amp 1 \\
0 \amp -1 \amp 0 \amp 2 \amp 0 \amp 0
\end{array}\right]
    </me>
</p>
<sage>
    <input>
        P=matrix([(1, 0, 1, 1, 0, 1),
 (-1, -3, 0, 5, -3, 1),
 (-1, -2, 0, 4, 0, 0),
 (0, 0, 0, 1, -1, 1),
 (0, -2, 0, 5, 0, 1),
 (0, -1, 0, 2, 0, 0)])
 show(P^-1*A*P)
    </input>
    <output>
        
    </output>
</sage>
<p>
    We did it!
</p>
</statement>

</example>

</section>