<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="s_diagable" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Diagonalizable operators</title>
    <p>
      Our treatment of eigenvectors in <xref ref="s_eigenvector"/> was motivated in part by the objective of finding particularly simple matrix representations <m>[T]_B</m> of a linear transformation <m>T\colon V\rightarrow V</m>. The simplest situation we could hope for is that there is a choice of basis <m>B</m> for which <m>[T]_B</m> is diagonal. We say that the basis <m>B</m> <em>diagonalizes</em> the transformation <m>T</m> in this case, and that <m>T</m> is <em>diagonalizable</em>. In this section we develop theoretical and computational tools for determining whether a linear transformation <m>T</m> is diagonalizable, and for finding a diagonalizing basis <m>B</m> when <m>T</m> is in fact diagonalizable.
        </p>
    <definition xml:id="d_diagonalizable">
    <title>Diagonalizable</title>
    <statement>
    <p>
    Let <m>V</m> be a finite-dimensional vector space. A linear operator <m>T\in \mathcal{L}(V)</m> is <term>diagonalizable</term> if there exists a basis <m>B</m> of <m>V</m> such that <m>[T]_B</m> is a diagonal matrix. In this case, we say that the basis <m>B</m> <term>diagonalizes</term> <m>T</m>.
    </p>
    </statement>
    </definition>
    <convention>
        <title>Diagonalizable matrix</title>
        <p>
            We will say a matrix <m>A\in F^{n,n}</m> is diagonalizable if its corresponding matrix transformation <m>T_A\colon F^{n}\rightarrow F^{n}</m> is diagonalizable. 
        </p>
        <p>
            Let's examine what this means in terms of matrix arithmetic. Let <m>B=(\bolde_1,\bolde_2,\dots, \bolde_n)</m> be the standard basis of <m>F^n</m>, and recall that <m>A=[T_A]_B</m>: <ie/>, the standard matrix of <m>T=T_A</m> is the matrix representation of <m>T_A</m> with respect to the standard basis. The matrix <m>A</m> is diagonalizable if and only if there is a basis <m>B'</m> of <m>F^{n}</m> such that <m>[T_A]_{B'}=D</m> is a diagonal matrix. Using <xref ref="th_similarity_matrixreps"/>, this in turn is equivalent to there being an invertible matrix <m>P</m> such that 
            <men xml:id="eq_matrix_diagonalize">
                D=P^{-1}AP
            </men>.
            Thus, we conclude that <m>A</m> is diagonalizable if and only if it is similar to a diagonal matrix. We say that the invertible matrix <m>P</m> in <xref ref="eq_matrix_diagonalize"/> <em>diagonalizes</em> the matrix <m>A</m>. 
        </p>
    </convention>
    <p>
        Given a basis <m>B=(v_1,v_2,\dots, v_n)</m> of <m>V</m> and linear operator <m>T\in \mathcal{L}(V)</m>, looking at the recipe for the matrix representation <m>[T]_B</m>, we see that we have <m>[T]_B=D</m> for some diagonal matrix 
        <me>
            D=\begin{bmatrix}
            d_1 \amp 0\amp \dots \amp \amp 0\\
            0   \amp d_2\amp 0\amp \dots \amp 0\\
            0\amp 0\amp \ddots \amp \amp 0\\
            \vdots  \amp \amp \amp \amp \vdots \\
            0\amp 0\amp \dots \amp 0\amp d_n
        \end{bmatrix}
        </me>
        if and only if 
        <md>
            <mrow>\begin{amatrix}[cccc]\vert \amp \vert \amp \amp \vert \\[20pt]
                \left[T(v_1)\right]_{B}\amp [T(v_2)]_{B}\amp \dots \amp [T(v_n)]_{B} \\[20pt]
                \vert \amp \vert \amp  \amp \vert\\[20pt]
              \end{amatrix} =
                \begin{bmatrix}
                d_1 \amp 0\amp \dots \amp \amp 0\\
                0   \amp d_2\amp 0\amp \dots \amp 0\\
                0\amp 0\amp \ddots \amp \amp 0\\
                \vdots  \amp \amp \amp \amp \vdots \\
                0\amp 0\amp \dots \amp 0\amp d_n
            \end{bmatrix}
            </mrow>
        </md>,
        if and only if 
        <me>
            [T(v_j)]_B=(0,0,\dots, d_j,0,\dots, 0)
        </me>
        for all <m>1\leq j\leq n</m>, if and only if 
        <me>
            T(v_j)=d_jv_j
        </me>
        for all <m>1\leq j\leq n</m>. Since vectors of a basis are necessarily nonzero, we conclude that <m>B</m> diagonalizes <m>T</m> if and only if it consists of eigenvectors of <m>T</m>. We call such a basis an <em>eigenbasis</em>. 
    </p>
    <definition xml:id="d_eigenbasis">
    <title>Eigenbasis</title>
    <statement>
    <p>
    Let <m>T\in \mathcal{L}(V)</m>. A basis <m>B=(v_i)_{i\in I}</m> of <m>V</m> is an <term>eigenbasis</term> of <m>T</m> if <m>v_i</m> is an eigenvector of <m>T</m> for all <m>i\in I</m>. 
    </p>
    </statement>
    </definition>
    <p>
        Our discussion above <xref ref="d_eigenbasis"/> constitutes a proof of the first two statements in <xref ref="th_diagable"/>.  
    </p>
    <theorem xml:id="th_diagable">
    <title>Diagonalizable operators</title>
    <statement>
    <p>
    Let <m>T</m> be a linear operator on the finite-dimensional vector space <m>V</m>, and let <m>\lambda_1,\lambda_2,\dots, \lambda_r</m> be the distinct eigenvalues of <m>T</m>. The following statements are equivalent. 
    <ol>
        <li>
            <p>
                <m>T</m> is diagonalizable. 
            </p>
        </li>
        <li>
            <p>
                <m>T</m> has an eigenbasis <m>B</m>.  
            </p>
        </li>
        <li>
            <p>
                <m>V=\bigoplus_{i=1}^r E(\lambda_i, T)</m>.
            </p>
        </li>
        <li>
            <p>
                <m>\dim V=\sum_{i=1}^r\dim E(\lambda_i, T)</m>.
            </p>
        </li>
    </ol>
    </p>
    </statement>
    </theorem>
    <p>
        We defer a full proof of <xref ref="th_diagable"/> for the time being, opting instead to illustrate with some examples how we can decide whether a linear transformation is diagonalizable.  The last statement of <xref ref="th_diagable"/> will serve as the foundation for our computational technique for answering such questions, as we make official in the following procedure.
    </p>
    <algorithm xml:id="proc_diagable">
        <title>Diagonalizable operator</title>
        <statement>
            <p>
                Let <m>T</m> be a linear operator of the finite-dimensional vector space <m>V</m>. To determine whether <m>T</m> is diagonalizable and/or find an eigenbasis of <m>T</m>, proceed as follows. 
                <ol>
                    <li>
                        <p>
                            Determine the distinct eigenvalues <m>\lambda_1,\lambda_2,\dots, \lambda_r</m> of <m>T</m>. 
                        </p>
                    </li>
                    <li>
                        <p>
                            For each <m>1\leq i\leq r</m>, compute a basis <m>B_i</m> for <m>E(\lambda_i, T)</m>. 
                        </p>
                    </li>
                    <li>
                        <p>
                            <m>T</m> is diagonalizable if and only if <m>\sum_{i=1}^r\dim E(\lambda_i, T)=\dim V</m>. 
                        </p>
                    </li>
                    <li>
                        <p>
                            If <m>T</m> is diagonalizable, then the tuple <m>B=B_1*B_2*\cdots *B_r</m> obtained by concatenating the bases <m>B_i</m> is an eigenbasis of <m>T</m>. 
                        </p>
                    </li>
                </ol>
            </p>
        </statement>
    </algorithm>
    <example xml:id="eg_diagable_transposition">
    <title>Transposition</title>
    <statement>
    <p>
    Let <m>S\colon F^{2,2}\rightarrow F^{2,2}</m> be defined as <m>S(A)=A^T</m>. Decide whether <m>S</m> is diagonalizable; if it is, find an eigenbasis <m>B</m> of <m>S</m> and compute <m>[S]_B</m>. 
    </p>
    </statement>
    <solution>
    <p>
    We have already done most of the work of <xref ref="proc_diagable"/> for this linear operator in <xref ref="eg_transposition_eigenspace"/>. There we saw that <m>1</m> and <m>-1</m> are the only eigenvalues of <m>S</m>, and the the bases of <m>E(1,S)</m> and <m>E(-1,S)</m> are 
    <md>
        <mrow>B_1\amp =\left( \begin{bmatrix}
            1\amp 0\\ 0 \amp 0
          \end{bmatrix},
          \begin{bmatrix}
            0\amp 1\\ 1 \amp 0
          \end{bmatrix},
          \begin{bmatrix}
            0\amp 0\\ 0 \amp 1
          \end{bmatrix}\right) \amp </mrow>
          <mrow>B_2 \amp =\left(\begin{bmatrix}
          0\amp 1\\ -1\amp 0
          \end{bmatrix} \right)</mrow>
    </md>,
    respectively. It follows that 
    <md>
        <mrow>\dim E(1,S) \amp =\len B_1=3</mrow>
        <mrow>\dim E(-1,S) \amp =\len B_2=1</mrow>
    </md>.
    Since we have 
    <me>
        \dim F^{2,2}=4=3+1=\dim E(1,S)+\dim E(-1,S)
    </me>,
    we conclude that <m>S</m> is diagonalizable, and furthermore that 
    <me>
        B=\left(\begin{bmatrix}
        1\amp 0\\ 0 \amp 0
      \end{bmatrix},
      \begin{bmatrix}
        0\amp 1\\ 1 \amp 0
      \end{bmatrix},
      \begin{bmatrix}
        0\amp 0\\ 0 \amp 1
      \end{bmatrix},  \begin{bmatrix}
      0\amp 1\\ -1\amp 0
      \end{bmatrix}\right)
    </me>
    is an eigenbasis of <m>S</m>.
    </p>
    </solution>
    </example>
    <example xml:id="eg_nondiagable_matrix_transform">
    <title>Matrix transformation</title>
    <statement>
    <p>
    Let <m>T\colon \R^3\rightarrow \R^3</m> be the matrix transformation <m>T=T_A</m>, where 
    <me>
        A=\begin{bmatrix}
        1\amp 1\amp 0 \\0\amp 1\amp 0\\ 0\amp 0\amp 2
        \end{bmatrix}
    </me>.
    Decide whether <m>T_A</m> is diagonalizable; if it is, find an invertible matrix <m>P</m> that diagonalizes <m>A</m> and compute <m>D=P^{-1}AP</m>. 
    
    </p>
    </statement>
    <solution>
    <p>
    We compute the characteristic polynomial <m>f</m> as 
    <md>
        <mrow>f(x) \amp = \det(xI-A)</mrow>
        <mrow> \amp = \det \begin{amatrix}[ccc]
        x-1\amp -1 \amp 0\\ 0 \amp x-1\amp 0 \\ 0 \amp 0\amp x-2
        \end{amatrix} </mrow>
        <mrow> \amp = (x-1)^2(x-2)</mrow>
    </md>.
    (Note how easy <m>f</m> was to compute, thanks to <m>A</m> being upper triangular.) The distinct eigenvalues of <m>A</m> are thus <m>1</m> and <m>2</m>. We compute 
    <md>
        <mrow>E(1,T_A) \amp = \NS(I-A)</mrow>
        <mrow> \amp =\NS \begin{amatrix}[ccc]
        0\amp -1 \amp 0\\ 0 \amp 0\amp 0 \\ 0 \amp 0\amp -1
        \end{amatrix}</mrow>
        <mrow> \amp = \Span((1,0,0))</mrow>
        <mrow>E(2,T_A) \amp = \NS(2I-A)</mrow>
        <mrow> \amp =\NS \begin{amatrix}[ccc]
        1\amp -1 \amp 0\\ 0 \amp 1\amp 0 \\ 0 \amp 0\amp 0
        \end{amatrix}</mrow>
        <mrow> \amp = \Span((0,0,1))</mrow>
    </md>.
    Since 
    <md>
        <mrow>\dim E(1,T_A)+\dim E(2,T_A) \amp = 1+1=2\ne 3</mrow>
    </md>,
    we conclude that <m>T_A</m> is not diagaonalizable. 
    </p>
    </solution>
    </example>
    <example xml:id="eg_diagable_matrix_transform">
        <title>Matrix transformation</title>
        <statement>
        <p>
        Let <m>T\colon \R^3\rightarrow \R^3</m> be the matrix transformation <m>T=T_A</m>, where 
        <me>
            A=\begin{bmatrix}
            1\amp 0\amp 1 \\0\amp 1\amp 0\\ 0\amp 0\amp 2
            \end{bmatrix}
        </me>.
        Decide whether <m>T_A</m> is diagonalizable; if it is, find an invertible matrix <m>P</m> that diagonalizes <m>A</m> and compute <m>D=P^{-1}AP</m>. 
        
        </p>
        </statement>
        <solution>
        <p>
    We compute the characteristic polynomial <m>f</m> as 
    <md>
        <mrow>f(x) \amp = \det(xI-A)</mrow>
        <mrow> \amp = \det \begin{amatrix}[ccc]
        x-1\amp 0 \amp -\\ 0 \amp x-1\amp 0 \\ 0 \amp 0\amp x-2
        \end{amatrix} </mrow>
        <mrow> \amp = (x-1)^2(x-2)</mrow>
    </md>.
    The distinct eigenvalues of <m>A</m> are thus <m>1</m> and <m>2</m>. We compute 
    <md>
        <mrow>E(1,T_A) \amp = \NS(I-A)</mrow>
        <mrow> \amp =\NS \begin{amatrix}[ccc]
        0\amp 0\amp -1\\ 0 \amp 0\amp 0 \\ 0 \amp 0\amp -1
        \end{amatrix}</mrow>
        <mrow> \amp = \Span((1,0,0), (0,1,0))</mrow>
        <mrow>E(2,T_A) \amp = \NS(2I-A)</mrow>
        <mrow> \amp =\NS \begin{amatrix}[ccc]
        1\amp 0 \amp -1\\ 0 \amp 1\amp 0 \\ 0 \amp 0\amp 0
        \end{amatrix}</mrow>
        <mrow> \amp = \Span((1,0,1))</mrow>
    </md>.
    Since 
    <md>
        <mrow>\dim E(1,T_A)+\dim E(2,T_A) \amp = 2+1=3</mrow>
    </md>,
    we conclude that <m>T_A</m> is diagaonalizable. Furthermore we obtain an eigenbasis 
    <me>
        B'=((1,0,0),(0,1,0),(1,0,1))
    </me>
    by concantenating the bases for our two eigenspaces. Let <m>B</m> be the standard basis of <m>\R^3</m>. By the change of basis formula, we have 
    <me>
        [T_A]_{B'}=(\underset{B'\to B}{P})^{-1}[T_A]_B\underset{B'\to B}{P}=(\underset{B'\to B}{P})^{-1}A\underset{B'\to B}{P}
    </me>.
    Since <m>B'</m> is an eigenbasis, we easily compute 
    <me>
        D=[T_A]_{B'}=\begin{bmatrix}
        1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 0\amp 0\amp 2
        \end{bmatrix}
    </me>.
    Thus we have 
    <me>
        D=P^{-1}AP, \hspace{10pt} P=\underset{B'\to B}{P}=\begin{bmatrix}
        1\amp 0\amp 0\\ 0\amp 1\amp 0\\ 1\amp 0\amp 1
        \end{bmatrix}
    </me>.
    </p>
    </solution>
        </example>
<p>
    The next example is a first glimpse at the algebraic usefulness of diagonalizability. If <m>A</m> is diagonalizable, we have 
    <md>
        <mrow>D \amp =P^{-1}AP \amp A=PDP^{-1}</mrow>
    </md>
    and hence 
    <md>
        <mrow>A^n \amp =(PDP^{-1})^n</mrow>
        <mrow> \amp =PD^nP^{-1}</mrow>
    </md>
    for any positive integer <m>n</m>. This gives us a convenient means of computing powers of diagonalizable matrices.
</p>
<example xml:id="eg_matrix_power">
<title>Matrix power</title>
<statement>
<p>
Let <m>A=\begin{bmatrix}
3\amp 1\\ 1\amp 3
\end{bmatrix}</m>. Compute <m>A^{100}</m>
</p>
</statement>
<solution>
<p>
The characteristic polynomial of <m>A</m> is <m>f(x)=x^2-6x+8=(x-4)(x-2)</m>. The usual computation shows that 
<md>
    <mrow>E(2, T_A) \amp = \Span ((1,-1)) </mrow>
    <mrow>E(4,T_A) \amp =\Span ((1,1))</mrow>
</md>,
and thus that we have <m>D=P^{-1}AP</m>, where 
<me>
    D=\begin{bmatrix}
    2\amp 0 \\ 0\amp 4
    \end{bmatrix},
    P=\begin{bmatrix}
    1\amp 1 \\ -1\amp 1
    \end{bmatrix}
</me>.
From the discussion above, we conclude that 
<md>
    <mrow>A^{100} \amp =(PDP^{-1})^{100}</mrow>
    <mrow> \amp = PD^{100}P^{-1}</mrow>
    <mrow> \amp =\begin{bmatrix}
    1\amp 1 \\ -1\amp 1
    \end{bmatrix}
\begin{bmatrix}
    2^{100}\amp 0 \\ 0\amp 4^{100}
    \end{bmatrix}
    \begin{bmatrix}
    1/2\amp -1/2 \\ 1/2\amp 1/2
    \end{bmatrix}
</mrow>
<mrow> \amp = 
   \begin{bmatrix}
    1\amp 1 \\ -1\amp 1
    \end{bmatrix}
   \begin{bmatrix}
    2^{99}\amp -2^{99} \\ 2^{199}\amp 2^{199}
    \end{bmatrix}
</mrow>
<mrow> \amp = 
    \begin{bmatrix}
    2^{199}+2^{99} \amp 2^{199}-2^{99} \\
    2^{199}-2^{99} \amp 2^{199}+2^{99}
    \end{bmatrix}
</mrow>
</md>.

</p>
</solution>
</example>

        <theorem xml:id="th_eigenvectors_ind">
        <title>Independence of eigenvectors</title>
        <statement>
        <p>
        Let <m>T</m> be a linear operator on the vector space <m>V</m> (not necessarily finite dimensional). If <m>S=(v_i)_{i\in I}</m> is a tuple of eigenvectors of <m>T</m> satisfying <m>T(v_i)=\lambda_i v_i</m> for all <m>i\in I</m> and <m>\lambda_i\ne \lambda_j</m> for all <m>i\ne j</m> (<ie/>, the eigenvalues are distinct), then <m>S</m> is linearly independent. 
        </p>
        </statement>
        <proof>
       <p>
      We prove the result by contradiction. If <m>S=(v_i)_{i\in I}</m> is linearly depenendent, then we can find a linearly dependent finite subtuple <m>S'=(v_{i_1}, v_{i_2},\dots, v_{i_r})</m> of minimal length <m>r</m>. Since each <m>v_{i_j}</m> is an eigenvector of <m>T</m>, we have 
      <me>
        T(v_{i_j})=\lambda_j v_{i_j}
      </me>
      for all <m>1\leq j\leq r</m>; since the eigenvalues are assumed to be distinct, we have <m>\lambda_j\ne \lambda_\ell</m> for <m>j\ne \ell</m>. 
    </p>
    <p>
        Note first that we must have <m>r\geq 2</m>: since eigenvectors are nonzero by definition, no subtuple of <m>S</m> of length 1 is linearly dependent. Since <m>S'</m> is linearly dependent we have
    <men xml:id="eq_independent_eigenvectors_A">
      c_1v_{i_1}+c_2v_{i_2}+\cdots +c_rv_{i_r}=\boldzero
    </men>,
    where <m>c_i\ne 0</m> for some <m>1\leq i\leq r</m>.  After reordering, we may assume without loss of generality that <m>c_1\ne 0</m>. Next we apply <m>T</m> to both sides of <xref ref="eq_independent_eigenvectors_A"/>:
    <mdn>
      <mrow>c_1v_{i_1}+c_2v_{i_2}\cdots +c_rv_{i_r}\amp=\boldzero </mrow>
      <mrow>T(c_1v_{i_1}+c_2v_{i_2}+\cdots +c_rv_{i_r})\amp=T(\boldzero)</mrow>
      <mrow> c_1T(v_{i_1})+c_2T(v_{i_2})+\cdots +c_rT(v_{i_r})\amp =\boldzero </mrow>
      <mrow xml:id="eq_independent_eigenvectors_B">c_1\lambda_1v_{i_1}+c_2\lambda_2v_{i_2}+\cdots +c_r\lambda_rv_{i_r}=\boldzero</mrow>
    </mdn>.
    From equation <xref ref="eq_independent_eigenvectors_A"/> and the equation in <xref ref="eq_independent_eigenvectors_B"/> we have
    <me>
      \lambda_r(c_1v_{i_1}+c_2v_{i_2}\cdots +c_rv_{i_r})- (c_1\lambda_1v_{i_1}+c_2\lambda_2v_{i_2}\cdots +c_r\lambda_rv_{i_r})=\boldzero
    </me>,
    and hence
    <men xml:id="eq_independent_eigenvectors_C">
      c_1(\lambda_r-\lambda_1)v_{i_1}+\cdots +c_{r-1}(\lambda_r-\lambda_{r-1})v_{i_{r-1}}+\cancel{c_r(\lambda_r-\lambda_r)}v_{i_r}=\boldzero
    </men>.
    Since <m>c_1\ne 0</m> and <m>\lambda_1\ne \lambda_r</m>, we have <m>c_1(\lambda_r-\lambda_1)\ne 0</m>. Thus equation <xref ref="eq_independent_eigenvectors_C"/> implies that the tuple <m>S''=(v_{i_1}, v_{i_2}, \dots, v_{i_{r-1}})</m> is a linearly dependent subtuple of <m>S</m> of length <m>r=1</m>, contradicting the minimality of <m>r</m>. Since we have reached a contradiction, we conclude that <m>S</m> must be linearly independent. 
    </p>
  </proof>
        </theorem>
        <example xml:id="eg_exponential_eigenvectors">
        <title>Exponential functions as eigenvectors</title>
        <statement>
        <p>
        Let <m>I</m> be an interval of <m>\R</m> containing more than one element. For all <m>\lambda\in \R</m>, define <m>f_\lambda\in C^\infty(\R)</m> as <m>f_\lambda(x)=e^{\lambda x}</m>. Prove that <m>S=(f_\lambda)_{\lambda\in \R}</m> is an uncountably infinite linear independent tuple in <m>C^{\infty}(I)</m>. 
        </p>
        </statement>
        <solution>
        <p>
        We saw in <xref ref="eg_eigenvectors_adhoc_derivative"/> that the functions <m>f_\lambda</m> are all eigenvectors of the differential operator <m>T\colon C^\infty(I)\rightarrow C^\infty(I)</m>, <m>T(f)=f'</m>. Since <m>T(f_\lambda)=\lambda f_\lambda</m> for all <m>\lambda</m>, we see that the functions <m>f_\lambda</m> all have distinct eigenvalues. We conclude from <xref ref="th_eigenvectors_ind"/> that <m>(f_\lambda)_{\lambda\in \R}</m> is a linearly independent tuple. Since this tuple is indexed by <m>\R</m> itself, we see that it is an uncountably infinite linear independent tuple. 
        </p>
        </solution>
        </example>
        
        <corollary xml:id="cor_eigenvectors_ind">
            <title>Independence of eigenvectors</title>
            <statement>
                <p>
                    Let <m>T</m> be a linear operator on the vector space <m>V</m> (not necessarily finite dimensional). If <m>\lambda_1,\lambda_2,\dots, \lambda_r</m> are distinct eigenvalues of <m>T</m>, then 
                    <men xml:id="eq_eigenspace_directsum">
                        \sum_{i=1}^rE(\lambda_i, T)=\bigoplus_{i=1}^r E(\lambda_i, T)
                    </men>.
                </p>
            </statement>
            <proof>
                <p>
                    Suppose we have a decomposition of $\boldzero$ of the form  
                    <men xml:id="eq_proof_eigenspace_directsum">
                        \boldzero=v_1+v_2+\cdots +v_r, v_i\in E(\lambda_i, T)
                    </men>.
                    Let $J=\{j\in \{1,2,\dots, r\}\colon v_j\ne \boldzero\}$. If $J$ is nonempty, then $v_j$ is a $\lambda_j$-eigenvector, and we have 
                    <me>
                        \boldzero=\sum_{j\in J}v_j
                    </me>,
                    a nontrivial linear combination of the tuple <m>S=(v_j)_{j\in J}</m>. This is a contradiction, as this tuple of eigenvectors (with distinct eigenvalues) is linearly independent by <xref ref="th_eigenvectors_ind"/>. Thus <m>J</m> is empty, and we see that the decomposition <xref ref="eq_proof_eigenspace_directsum"/> must be the trivial one. We conclude from <xref ref="th_direct_sum_dim"/> that subspace sum <m>W=\sum_{i=1}^rE(\lambda_i, T)</m> is direct: <ie/>, that 
                    <me>
                        \sum_{i=1}^rE(\lambda_i, T)=\bigoplus_{i=1}^rE(\lambda_i, T)
                    </me>,
                    as desired.
                </p>
            </proof>
        </corollary>
        <p>
            <xref ref="th_eigenvectors_ind"/> and <xref ref="cor_eigenvectors_ind"/> give us the necessary theoretical tools to complete the proof of <xref ref="th_diagable"/>. 
        </p>
        <proof>
            <title>Proof of <xref ref="th_diagable"/></title>
            <p>
                Recall that we have already provided an argument proving that statements (1) and (2) of <xref ref="th_diagable"/> are equivalent. We will prove the cycle of implications (2)<m>\implies</m>(3)<m>\implies</m>(4)<m>\implies</m>(2)
            </p>
            <case>
            <title>Implication: (2)<m>\implies</m>(3)</title>
            <p>
            Assume <m>T</m> has an eigenbasis <m>B</m>. Since the elements of <m>B</m> are eigenvectors, after a re-ordering of <m>B</m>, we can assume <m>B</m> is of the form 
            <me>
                B=(v_{1,1},\dots, v_{1,n_1},v_{2,1},\dots, v_{2,n_2},\dots, v_{\ell,1},\dots, v_{\ell,n_{\ell}})
            </me>,
            where for all <m>1\leq i\leq \ell</m> and <m>1\leq j\leq n_i</m>, <m>v_{i,j}</m> is an eigenvector of <m>T</m> with eigenvalue <m>\lambda_i</m>. (Note that we do not assume that <m>\ell=r</m> here, but it follows from the rest of our argument that  that this must be the case.) Since <m>B</m> is a basis, given any <m>v\in V</m>, we have 
            <md>
                <mrow>v \amp =\underset{w_1}{\underbrace{\sum_{j=1}^{n_1}v_{1,j}}}+\underset{w_2}{\underbrace{\sum_{j=1}^{n_2}v_{2,j}}}+\cdots +\underset{w_\ell}{\underbrace{\sum_{j=1}^{n_\ell}v_{\ell,j}}}</mrow>
                <mrow> \amp =w_1+w_2+\cdots +w_\ell</mrow>
            </md>,
            where <m>w_i=\sum_{j=1}^{n_i}v_{i,j}\in E(\lambda_i, T)</m> for all <m>1\leq i\leq \ell</m>. This proves <m>V=\sum_{i=1}^\ell E(\lambda_i, T)</m>. By <xref ref="cor_eigenvectors_ind"/>, we conclude that <m>V=\bigoplus_{i=1}^\ell E(\lambda_i, T)</m>. Lastly, since clearly  
            <me>
                \sum_{i=1}^\ell E(\lambda_i, T)\subseteq \sum_{i=1}^rE(\lambda_i, T)
            </me>,
            we have 
            <md>
                <mrow>V \amp =\sum_{i=1}^rE(\lambda_i, T)</mrow>
                <mrow> \amp = \bigoplus_{i=1}^rE(\lambda_i, T) \amp (<xref ref="cor_eigenvectors_ind" text="global"/>)</mrow>
            </md>,
            as desired. 
            </p>
            </case>
            <case>
            <title>Implication: (3)<m>\implies</m>(4)</title>
            <p>
            This is a direct consequence of <xref ref="th_direct_sum_dim"/>. 
            </p>
            </case>
            <case>
            <title>Implication: (4)<m>\implies</m>(2)</title>
            <p>
            Assume <m>\sum_{i=1}^r\dim E(\lambda_i, T)=\dim V=n</m>. For each <m>1\leq i\leq r</m>, let 
            <me>
                B_i=(v_{i,1}, v_{i,2},\dots, v_{i,n_{i}})
            </me>
            be a basis of <m>E(\lambda_i, T)</m>. Since <m>\len B_i=n_i</m>, we see that 
            <me>
                \sum_{i=1}^rn_i=\sum_{i=1}^{r}\dim E(\lambda_i, T)=n
            </me>.
            Consider the concatentation <m>B=B_1*B_2*\cdots * B_r</m> of the bases <m>B_i</m>: <ie/>, 
            <me>
                B=(v_{1,1},\dots, v_{1,n_1}, v_{2,1},\dots, v_{2,n_2},\dots, v_{r,1},\dots, v_{r,n_r})
            </me>.
            Since each <m>B_i</m> is the basis of <m>E(\lambda_i, T)</m>, and since 
            <me>
                \sum_{i=1}^rE(\lambda_i, T)=\bigoplus_{i=1}^rE(\lambda_i, T)
            </me>
            by <xref ref="cor_eigenvectors_ind"/>, it follows that <m>B</m> is linearly independent. (This is left as an exercise.) Since <m>\len B=n=\dim V</m>, it follows from <xref ref="cor_street_smarts"/> that <m>B</m> is a basis of <m>V</m>. Since each element of <m>B</m> is an eigenvector of <m>T</m>, we conclude that <m>B</m> is an eigenbasis of <m>T</m>. 
            </p>
            </case>
            
            
        </proof>


        <corollary xml:id="cor_distinct_eigenvalues">
            <title>Distinct eigenvalues</title>
            <statement>
                <p>
                    Let <m>T\in \mathcal{L}(V)</m>, where <m>\dim V=n&lt; \infty</m>. If <m>T</m> has <m>n</m> distinct eigenvalues, then <m>T</m> is diagonalizable. 
                </p>
            </statement>
            <proof>
                <p>
                    Let <m>\lambda_1,\lambda_2,\dots, \lambda_n</m> be the <m>n</m> distict eigenvalues of <m>T</m>, and let <m>B=(v_1,v_2,\dots, v_n)</m> be a tuple of eigenvectors satisfying <m>T(v_i)=\lambda_i</m> for all <m>1\leq i\leq n</m>. By <xref ref="th_eigenvectors_ind"/>, <m>B</m> is linearly independent. Since <m>\dim V=n=\len B</m>, it follows that <m>B</m> is an eigenbasis of <m>V</m>. From <xref ref="th_diagable"/>, we conclude that <m>T</m> is diagonalizable. 
                </p>
            </proof>
        </corollary>
        <remark>
            <title>Distinct eigenvalues</title>
            <p>
                Mark well that the implication in <xref ref="cor_distinct_eigenvalues"/> is not an if and only if! Consider the identity transformation <m>I_V\colon V\rightarrow V</m> on a nonzero finite-dimensional vector space <m>V</m>. Given any basis <m>B</m> of <m>V</m>, we have 
                <me>
                    [I_V]_B=I\in F^{n,n}
                </me>.
                Thus <m>I_V</m> is diagonalizable. And yet, <m>I_V</m> has only one eigenvalue, namely <m>\lambda=1</m>.
            </p>
        </remark>
        <example xml:id="eg_distinct_eigenvalues_complex">
        <title>Complex linear operator</title>
        <statement>
        <p>
        Assume <m>T</m> is a linear operator of the 3-dimensional <m>\C</m>-vector space <m>V</m> and that the characteristic polynomial of <m>T</m> is <m>f(x)=x^3-1</m>. Decide whether <m>T</m> is diagonalizable. 
        </p>
        </statement>
        <solution>
        <p>
        Over <m>\C</m> the characteristic polynomial factors as
        <me>
            f(x)=(x-1)(x^2+x+1)=(x-1)(x-\alpha)(x-\beta)
        </me>,
        where 
        <md>
            <mrow>\alpha \amp =-\frac{1}{2}+\frac{\sqrt{3}}{2}i</mrow>
            <mrow>\beta \amp =-\frac{1}{2}-\frac{\sqrt{3}}{2}i</mrow>
        </md>.
        Thus <m>T</m> has three <em>distinct</em> eigenvalues <m>1, \alpha, \beta</m>. Since <m>\dim V=3</m>,  it follows from <xref ref="cor_distinct_eigenvalues"/> that <m>T</m> is diagonalizable. 
        </p>
        </solution>
        </example>
        <example xml:id="eg_distinct_eigenvalues_real">
            <title>Real linear operator</title>
            <statement>
            <p>
            Assume <m>T</m> is a linear operator of the 3-dimensional <m>\R</m>-vector space <m>V</m>, that the characteristic polynomial of <m>T</m> is <m>f(x)=x^3-1</m>, and that <m>T\ne I_V</m>. Decide whether <m>T</m> is diagonalizable. 
            </p>
            </statement>
            <solution>
            <p>
                Over <m>\R</m> the characteristic polynomial <m>f</m> factors as 
                <me>
                    f(x)=(x-1)(x^2+x+1)
                </me>.
                Thus <m>1</m> is the only eigenvalue of <m>T</m>. If <m>T</m> were diagonalizable, we would have 
                <me>
                    V=\bigoplus E(\lambda_i, T)=E(1,T)
                </me>. 
                But then <m>v\in E(1,T)</m> for all <m>v\in V</m>, in which case 
                <me>
                    T(v)=1v=v
                </me>
                for all <m>v\in V</m>: <ie/>, we would have <m>T=I_V</m> in this case. We conclude that <m>T</m> is not diagonalizable.
            </p>
            </solution>
            </example>
        

</section>